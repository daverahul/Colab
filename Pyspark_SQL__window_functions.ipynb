{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daverahul/Colab/blob/main/Pyspark_SQL__window_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing pyspark and creating a spark session."
      ],
      "metadata": {
        "id": "e_5s4NUTOkJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C62Jl4Q_OXbV"
      },
      "outputs": [],
      "source": [
        "## pip install pyspark\n",
        "!pip install -q pyspark\n",
        "\n",
        "## Creating Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession.builder.appName(\"test-app\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover some window functions in pyspark SQL\n",
        "\n",
        "There are mainly two types of Window functions. We will see examples of each.\n",
        "\n",
        "1. Ranking Function\n",
        "2. Aggregate Function\n"
      ],
      "metadata": {
        "id": "C2U0EeX0O6rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets create some data first\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "salaryData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"Jordon\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "df_salary = spark.createDataFrame(data = salaryData, schema = columns)\n",
        "df_salary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS6yGMfDPFq5",
        "outputId": "c63bdceb-5b30-4bda-e247-45735dbcc43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|       Jordon|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RANKING WINDOW FUNCTION"
      ],
      "metadata": {
        "id": "IjhOYTBFPgry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## One of the very simple example is to use row_number based partitioned by a column and group by another column\n",
        "## In this example we take department as the grouping column, and salary as sorting column\n",
        "\n",
        "## USING ROW_NUMBER()\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df_salary.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qJ1qTTkPTOe",
        "outputId": "1ab61d2e-203d-4e0b-825f-df9c8e39b4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Jordon       |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On above example as you noticed in the Sales Department , James and Jordon has same salary of 3000, Robert and Saif has same salary of 4100; however they are ranked differently.\n",
        "\n",
        "If we want to have the same rank when there is a TIE, we can use ranking. There are two types of ranking when it comes to Window Functioning:\n",
        "\n",
        "rank() - It breaks the tie, but leaves GAP in the sequence when it finds tie.\n",
        "dense_rank() - It breaks tie without leaving any gaps in the sequence.\n",
        "Lets see the examples on same dataset."
      ],
      "metadata": {
        "id": "TCqwp7FkQ1Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## USING RANK()\n",
        "\n",
        "df_salary.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TyhLRLbQ4Rw",
        "outputId": "64a9ce98-0902-4a52-e11d-fc42fe8e201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|Maria        |Finance   |3000  |1   |\n",
            "|Scott        |Finance   |3300  |2   |\n",
            "|Jen          |Finance   |3900  |3   |\n",
            "|Kumar        |Marketing |2000  |1   |\n",
            "|Jeff         |Marketing |3000  |2   |\n",
            "|James        |Sales     |3000  |1   |\n",
            "|Jordon       |Sales     |3000  |1   |\n",
            "|Robert       |Sales     |4100  |3   |\n",
            "|Saif         |Sales     |4100  |3   |\n",
            "|Michael      |Sales     |4600  |5   |\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## USING DENSE_RANK()\n",
        "df_salary.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxoL8PL7RAom",
        "outputId": "f30f2919-0741-4fc9-b242-5ed52c563a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Jordon       |Sales     |3000  |1         |\n",
            "|Robert       |Sales     |4100  |2         |\n",
            "|Saif         |Sales     |4100  |2         |\n",
            "|Michael      |Sales     |4600  |3         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AGGREGATE WINDOW FUNCTION"
      ],
      "metadata": {
        "id": "yYY0tiJ0RfB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the popular ones are: sum, avg, min , max"
      ],
      "metadata": {
        "id": "tRkMmgLQRsLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Aggregate functions\n",
        "## Lets see what are the average, sum, min, max salaries across different departments\n",
        "\n",
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
        "df_salary.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFWdusfTRGnf",
        "outputId": "16b0ebde-6ece-4142-e9c9-ec4dce502fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stopping spark session- its a good practice to kill the spark session once done\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "SrLDilWZRvPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a Python function\n",
        "def double_age(age):\n",
        "    return age * 2\n",
        "\n",
        "# Register the Python function as a UDF\n",
        "double_age_udf = udf(double_age, IntegerType())\n",
        "\n",
        "# Apply the UDF to a DataFrame\n",
        "df_result = df.withColumn(\"DoubleAge\", double_age_udf(\"Age\"))\n",
        "\n",
        "# Show the result\n",
        "df_result.show()\n"
      ],
      "metadata": {
        "id": "xydSEQijSFfG",
        "outputId": "4c381cf4-3197-4b07-d4da-e0e2570aaf14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------+\n",
            "| Name|Age|DoubleAge|\n",
            "+-----+---+---------+\n",
            "| John| 25|       50|\n",
            "|Alice| 30|       60|\n",
            "|  Bob| 22|       44|\n",
            "+-----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, round\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"average_selling_price\").getOrCreate()\n",
        "\n",
        "# Data for Prices and Units Sold\n",
        "prices_data = [(1, datetime.date(2019, 2, 17), datetime.date(2019, 2, 28), 5),\n",
        "(1, datetime.date(2019, 3, 1), datetime.date(2019, 3, 22), 20),\n",
        "(2, datetime.date(2019, 2, 1), datetime.date(2019, 2, 20), 15),\n",
        "(2, datetime.date(2019, 2, 21), datetime.date(2019, 3, 31), 30)]\n",
        "\n",
        "units_sold_data = [(1, datetime.date(2019, 2, 25), 100),\n",
        "(1, datetime.date(2019, 3, 1), 15),\n",
        "(2, datetime.date(2019, 2, 10), 200),\n",
        "(2, datetime.date(2019, 3, 22), 15)]\n",
        "\n",
        "# Schemas\n",
        "prices_schema = StructType([\n",
        "StructField(\"product_id\", IntegerType(), True),\n",
        "StructField(\"start_date\", DateType(), True),\n",
        "StructField(\"end_date\", DateType(), True),\n",
        "StructField(\"price\", IntegerType(), True)\n",
        "])\n",
        "units_sold_schema = StructType([\n",
        "StructField(\"product_id\", IntegerType(), True),\n",
        "StructField(\"purchase_date\", DateType(), True),\n",
        "StructField(\"units\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "df_prices = spark.createDataFrame(data=prices_data, schema=prices_schema)\n",
        "df_units_sold = spark.createDataFrame(data=units_sold_data, schema=units_sold_schema)\n"
      ],
      "metadata": {
        "id": "8nILUaN03y8Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prices.show()"
      ],
      "metadata": {
        "id": "eNFnlr5BFvtm",
        "outputId": "924529a7-126f-4d57-f474-7c95f1a1f0b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|product_id|start_date|  end_date|price|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2019-02-17|2019-02-28|    5|\n",
            "|         1|2019-03-01|2019-03-22|   20|\n",
            "|         2|2019-02-01|2019-02-20|   15|\n",
            "|         2|2019-02-21|2019-03-31|   30|\n",
            "+----------+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_units_sold.show()"
      ],
      "metadata": {
        "id": "N5o2k_LbGH7Y",
        "outputId": "5f30ac31-f641-4ca6-f1dd-0b5c3d11382a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----+\n",
            "|product_id|purchase_date|units|\n",
            "+----------+-------------+-----+\n",
            "|         1|   2019-02-25|  100|\n",
            "|         1|   2019-03-01|   15|\n",
            "|         2|   2019-02-10|  200|\n",
            "|         2|   2019-03-22|   15|\n",
            "+----------+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a solution in PySpark to find the average selling price for each product. average_price should be rounded to 2 decimal places."
      ],
      "metadata": {
        "id": "oD_cofd6GK0A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_prices.join(df_units_sold,\"product_id\",\"left\").where(col(\"purchase_date\").between(col(\"start_date\"),col(\"end_date\")))"
      ],
      "metadata": {
        "id": "aY9MK8iVGTg3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined.distinct().show()"
      ],
      "metadata": {
        "id": "LyqPiK-WGwIL",
        "outputId": "e8731811-8fec-4b03-c185-ca3222a53c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+-----+-------------+-----+\n",
            "|product_id|start_date|  end_date|price|purchase_date|units|\n",
            "+----------+----------+----------+-----+-------------+-----+\n",
            "|         1|2019-02-17|2019-02-28|    5|   2019-02-25|  100|\n",
            "|         1|2019-03-01|2019-03-22|   20|   2019-03-01|   15|\n",
            "|         2|2019-02-01|2019-02-20|   15|   2019-02-10|  200|\n",
            "|         2|2019-02-21|2019-03-31|   30|   2019-03-22|   15|\n",
            "+----------+----------+----------+-----+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_average = df_joined.groupby(\"product_id\").agg(\n",
        "    round(sum(col(\"units\") * col(\"price\"))/sum(\"units\"),2).alias(\"average_price\")\n",
        ")\n",
        "df_average.show()"
      ],
      "metadata": {
        "id": "gVZdFNuNG6VR",
        "outputId": "6cb297da-c894-4840-d6a7-697e99ef63a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|average_price|\n",
            "+----------+-------------+\n",
            "|         1|         6.96|\n",
            "|         2|        16.05|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_average_price = df_joined.groupby(\"product_id\").agg(\n",
        "round(sum(col(\"units\") * col(\"price\")) / sum(\"units\"), 2).alias(\"average_price\")\n",
        ")"
      ],
      "metadata": {
        "id": "z977fYGGIu9-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ask - You are provided with two datasets, branch1 and branch2 , representing information about stdents\n",
        "#and their marks in different subjects across different branches. Your goal is to combine these datasets\n",
        "#into one final dataset. Missing text information should be shown as 'unknown' , and\n",
        "#missing numerical information should be shown as -9999.\n",
        "\n",
        "#Pyspark code\n",
        "#--------------------\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Define the schema for branch1 DataFrame\n",
        "schema_branch1 = StructType([\n",
        "    StructField(\"Branch\", StringType(), True),\n",
        "    StructField(\"Student\", StringType(), True),\n",
        "    StructField(\"Maths_marks\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create branch1 DataFrame\n",
        "branch1_data = [(\"Delhi\", \"Neha\", 90)]\n",
        "branch1 = spark.createDataFrame(branch1_data, schema_branch1)\n",
        "\n",
        "# Define the schema for branch2 DataFrame\n",
        "schema_branch2 = StructType([\n",
        "    StructField(\"Student\", StringType(), True),\n",
        "    StructField(\"Branch\", StringType(), True),\n",
        "    StructField(\"Science_marks\", IntegerType(), True),\n",
        "    StructField(\"Maths_marks\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create branch2 DataFrame\n",
        "branch2_data = [\n",
        "    (\"Arav\", \"Kolkata\", 79, 83),\n",
        "    (None, \"Kolkata\", 89, 73)\n",
        "]\n",
        "branch2 = spark.createDataFrame(branch2_data, schema_branch2)\n",
        "#we are using unionByName to merge the two dataframes\n"
      ],
      "metadata": {
        "id": "P3hK-g7cJIEF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "branch1.show()"
      ],
      "metadata": {
        "id": "9d_NzY0jN7EM",
        "outputId": "6691f878-3d0b-43a7-a7b7-e2c6ed4f50c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----------+\n",
            "|Branch|Student|Maths_marks|\n",
            "+------+-------+-----------+\n",
            "| Delhi|   Neha|         90|\n",
            "+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "branch2.show()"
      ],
      "metadata": {
        "id": "2OyNqOMWOV3q",
        "outputId": "61fa6368-170f-41b0-aa52-b86721ea2daa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------+-----------+\n",
            "|Student| Branch|Science_marks|Maths_marks|\n",
            "+-------+-------+-------------+-----------+\n",
            "|   Arav|Kolkata|           79|         83|\n",
            "|   NULL|Kolkata|           89|         73|\n",
            "+-------+-------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "branch1_new = branch1.withColumn(\"Science_marks\",lit(None))"
      ],
      "metadata": {
        "id": "b67CoOWnOXvF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = branch2.unionByName(branch1_new)"
      ],
      "metadata": {
        "id": "TvLxIBvzOoM3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.dtypes"
      ],
      "metadata": {
        "id": "O0QImqZxQd5r",
        "outputId": "ae879bea-d51c-48dc-9820-5c752ce34d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Student', 'string'),\n",
              " ('Branch', 'string'),\n",
              " ('Science_marks', 'int'),\n",
              " ('Maths_marks', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name, col_types in merged.dtypes:\n",
        "  if col_types == 'int':\n",
        "    merged = merged.fillna({col_name:-9999})\n",
        "  elif col_types == 'string':\n",
        "    merged = merged.fillna({col_name:'unknown'})\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aw0RuSFIPJR7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.show()"
      ],
      "metadata": {
        "id": "pS5Ew_YZRCG_",
        "outputId": "a6a79d4d-76ee-4bb1-dfa1-2494da09bfb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------+-----------+\n",
            "|Student| Branch|Science_marks|Maths_marks|\n",
            "+-------+-------+-------------+-----------+\n",
            "|   Arav|Kolkata|           79|         83|\n",
            "|unknown|Kolkata|           89|         73|\n",
            "|   Neha|  Delhi|        -9999|         90|\n",
            "+-------+-------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21gokEMoSKL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}