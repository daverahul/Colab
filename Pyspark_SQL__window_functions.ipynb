{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daverahul/Colab/blob/main/Pyspark_SQL__window_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing pyspark and creating a spark session."
      ],
      "metadata": {
        "id": "e_5s4NUTOkJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C62Jl4Q_OXbV"
      },
      "outputs": [],
      "source": [
        "## pip install pyspark\n",
        "!pip install -q pyspark\n",
        "\n",
        "## Creating Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession.builder.appName(\"test-app\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover some window functions in pyspark SQL\n",
        "\n",
        "There are mainly two types of Window functions. We will see examples of each.\n",
        "\n",
        "1. Ranking Function\n",
        "2. Aggregate Function\n"
      ],
      "metadata": {
        "id": "C2U0EeX0O6rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets create some data first\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "salaryData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"Jordon\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "df_salary = spark.createDataFrame(data = salaryData, schema = columns)\n",
        "df_salary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS6yGMfDPFq5",
        "outputId": "c63bdceb-5b30-4bda-e247-45735dbcc43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|       Jordon|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RANKING WINDOW FUNCTION"
      ],
      "metadata": {
        "id": "IjhOYTBFPgry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## One of the very simple example is to use row_number based partitioned by a column and group by another column\n",
        "## In this example we take department as the grouping column, and salary as sorting column\n",
        "\n",
        "## USING ROW_NUMBER()\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df_salary.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qJ1qTTkPTOe",
        "outputId": "1ab61d2e-203d-4e0b-825f-df9c8e39b4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Jordon       |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On above example as you noticed in the Sales Department , James and Jordon has same salary of 3000, Robert and Saif has same salary of 4100; however they are ranked differently.\n",
        "\n",
        "If we want to have the same rank when there is a TIE, we can use ranking. There are two types of ranking when it comes to Window Functioning:\n",
        "\n",
        "rank() - It breaks the tie, but leaves GAP in the sequence when it finds tie.\n",
        "dense_rank() - It breaks tie without leaving any gaps in the sequence.\n",
        "Lets see the examples on same dataset."
      ],
      "metadata": {
        "id": "TCqwp7FkQ1Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## USING RANK()\n",
        "\n",
        "df_salary.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TyhLRLbQ4Rw",
        "outputId": "64a9ce98-0902-4a52-e11d-fc42fe8e201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|Maria        |Finance   |3000  |1   |\n",
            "|Scott        |Finance   |3300  |2   |\n",
            "|Jen          |Finance   |3900  |3   |\n",
            "|Kumar        |Marketing |2000  |1   |\n",
            "|Jeff         |Marketing |3000  |2   |\n",
            "|James        |Sales     |3000  |1   |\n",
            "|Jordon       |Sales     |3000  |1   |\n",
            "|Robert       |Sales     |4100  |3   |\n",
            "|Saif         |Sales     |4100  |3   |\n",
            "|Michael      |Sales     |4600  |5   |\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## USING DENSE_RANK()\n",
        "df_salary.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxoL8PL7RAom",
        "outputId": "f30f2919-0741-4fc9-b242-5ed52c563a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|Jordon       |Sales     |3000  |1         |\n",
            "|Robert       |Sales     |4100  |2         |\n",
            "|Saif         |Sales     |4100  |2         |\n",
            "|Michael      |Sales     |4600  |3         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AGGREGATE WINDOW FUNCTION"
      ],
      "metadata": {
        "id": "yYY0tiJ0RfB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the popular ones are: sum, avg, min , max"
      ],
      "metadata": {
        "id": "tRkMmgLQRsLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Aggregate functions\n",
        "## Lets see what are the average, sum, min, max salaries across different departments\n",
        "\n",
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
        "df_salary.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFWdusfTRGnf",
        "outputId": "16b0ebde-6ece-4142-e9c9-ec4dce502fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stopping spark session- its a good practice to kill the spark session once done\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "SrLDilWZRvPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"John\", 25), (\"Alice\", 30), (\"Bob\", 22)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a Python function\n",
        "def double_age(age):\n",
        "    return age * 2\n",
        "\n",
        "# Register the Python function as a UDF\n",
        "double_age_udf = udf(double_age, IntegerType())\n",
        "\n",
        "# Apply the UDF to a DataFrame\n",
        "df_result = df.withColumn(\"DoubleAge\", double_age_udf(\"Age\"))\n",
        "\n",
        "# Show the result\n",
        "df_result.show()\n"
      ],
      "metadata": {
        "id": "xydSEQijSFfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c381cf4-3197-4b07-d4da-e0e2570aaf14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------+\n",
            "| Name|Age|DoubleAge|\n",
            "+-----+---+---------+\n",
            "| John| 25|       50|\n",
            "|Alice| 30|       60|\n",
            "|  Bob| 22|       44|\n",
            "+-----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, round\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"average_selling_price\").getOrCreate()\n",
        "\n",
        "# Data for Prices and Units Sold\n",
        "prices_data = [(1, datetime.date(2019, 2, 17), datetime.date(2019, 2, 28), 5),\n",
        "(1, datetime.date(2019, 3, 1), datetime.date(2019, 3, 22), 20),\n",
        "(2, datetime.date(2019, 2, 1), datetime.date(2019, 2, 20), 15),\n",
        "(2, datetime.date(2019, 2, 21), datetime.date(2019, 3, 31), 30)]\n",
        "\n",
        "units_sold_data = [(1, datetime.date(2019, 2, 25), 100),\n",
        "(1, datetime.date(2019, 3, 1), 15),\n",
        "(2, datetime.date(2019, 2, 10), 200),\n",
        "(2, datetime.date(2019, 3, 22), 15)]\n",
        "\n",
        "# Schemas\n",
        "prices_schema = StructType([\n",
        "StructField(\"product_id\", IntegerType(), True),\n",
        "StructField(\"start_date\", DateType(), True),\n",
        "StructField(\"end_date\", DateType(), True),\n",
        "StructField(\"price\", IntegerType(), True)\n",
        "])\n",
        "units_sold_schema = StructType([\n",
        "StructField(\"product_id\", IntegerType(), True),\n",
        "StructField(\"purchase_date\", DateType(), True),\n",
        "StructField(\"units\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "df_prices = spark.createDataFrame(data=prices_data, schema=prices_schema)\n",
        "df_units_sold = spark.createDataFrame(data=units_sold_data, schema=units_sold_schema)\n"
      ],
      "metadata": {
        "id": "8nILUaN03y8Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prices.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNFnlr5BFvtm",
        "outputId": "924529a7-126f-4d57-f474-7c95f1a1f0b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+-----+\n",
            "|product_id|start_date|  end_date|price|\n",
            "+----------+----------+----------+-----+\n",
            "|         1|2019-02-17|2019-02-28|    5|\n",
            "|         1|2019-03-01|2019-03-22|   20|\n",
            "|         2|2019-02-01|2019-02-20|   15|\n",
            "|         2|2019-02-21|2019-03-31|   30|\n",
            "+----------+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_units_sold.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5o2k_LbGH7Y",
        "outputId": "5f30ac31-f641-4ca6-f1dd-0b5c3d11382a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-----+\n",
            "|product_id|purchase_date|units|\n",
            "+----------+-------------+-----+\n",
            "|         1|   2019-02-25|  100|\n",
            "|         1|   2019-03-01|   15|\n",
            "|         2|   2019-02-10|  200|\n",
            "|         2|   2019-03-22|   15|\n",
            "+----------+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a solution in PySpark to find the average selling price for each product. average_price should be rounded to 2 decimal places."
      ],
      "metadata": {
        "id": "oD_cofd6GK0A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_prices.join(df_units_sold,\"product_id\",\"left\").where(col(\"purchase_date\").between(col(\"start_date\"),col(\"end_date\")))"
      ],
      "metadata": {
        "id": "aY9MK8iVGTg3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined.distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyqPiK-WGwIL",
        "outputId": "e8731811-8fec-4b03-c185-ca3222a53c12"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+-----+-------------+-----+\n",
            "|product_id|start_date|  end_date|price|purchase_date|units|\n",
            "+----------+----------+----------+-----+-------------+-----+\n",
            "|         1|2019-02-17|2019-02-28|    5|   2019-02-25|  100|\n",
            "|         1|2019-03-01|2019-03-22|   20|   2019-03-01|   15|\n",
            "|         2|2019-02-01|2019-02-20|   15|   2019-02-10|  200|\n",
            "|         2|2019-02-21|2019-03-31|   30|   2019-03-22|   15|\n",
            "+----------+----------+----------+-----+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_average = df_joined.groupby(\"product_id\").agg(\n",
        "    round(sum(col(\"units\") * col(\"price\"))/sum(\"units\"),2).alias(\"average_price\")\n",
        ")\n",
        "df_average.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVZdFNuNG6VR",
        "outputId": "6cb297da-c894-4840-d6a7-697e99ef63a0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|average_price|\n",
            "+----------+-------------+\n",
            "|         1|         6.96|\n",
            "|         2|        16.05|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_average_price = df_joined.groupby(\"product_id\").agg(\n",
        "round(sum(col(\"units\") * col(\"price\")) / sum(\"units\"), 2).alias(\"average_price\")\n",
        ")"
      ],
      "metadata": {
        "id": "z977fYGGIu9-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ask - You are provided with two datasets, branch1 and branch2 , representing information about stdents\n",
        "#and their marks in different subjects across different branches. Your goal is to combine these datasets\n",
        "#into one final dataset. Missing text information should be shown as 'unknown' , and\n",
        "#missing numerical information should be shown as -9999.\n",
        "\n",
        "#Pyspark code\n",
        "#--------------------\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "# Define the schema for branch1 DataFrame\n",
        "schema_branch1 = StructType([\n",
        "    StructField(\"Branch\", StringType(), True),\n",
        "    StructField(\"Student\", StringType(), True),\n",
        "    StructField(\"Maths_marks\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create branch1 DataFrame\n",
        "branch1_data = [(\"Delhi\", \"Neha\", 90)]\n",
        "branch1 = spark.createDataFrame(branch1_data, schema_branch1)\n",
        "\n",
        "# Define the schema for branch2 DataFrame\n",
        "schema_branch2 = StructType([\n",
        "    StructField(\"Student\", StringType(), True),\n",
        "    StructField(\"Branch\", StringType(), True),\n",
        "    StructField(\"Science_marks\", IntegerType(), True),\n",
        "    StructField(\"Maths_marks\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create branch2 DataFrame\n",
        "branch2_data = [\n",
        "    (\"Arav\", \"Kolkata\", 79, 83),\n",
        "    (None, \"Kolkata\", 89, 73)\n",
        "]\n",
        "branch2 = spark.createDataFrame(branch2_data, schema_branch2)\n",
        "#we are using unionByName to merge the two dataframes\n"
      ],
      "metadata": {
        "id": "P3hK-g7cJIEF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "branch1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d_NzY0jN7EM",
        "outputId": "6691f878-3d0b-43a7-a7b7-e2c6ed4f50c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----------+\n",
            "|Branch|Student|Maths_marks|\n",
            "+------+-------+-----------+\n",
            "| Delhi|   Neha|         90|\n",
            "+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "branch2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OyNqOMWOV3q",
        "outputId": "61fa6368-170f-41b0-aa52-b86721ea2daa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------+-----------+\n",
            "|Student| Branch|Science_marks|Maths_marks|\n",
            "+-------+-------+-------------+-----------+\n",
            "|   Arav|Kolkata|           79|         83|\n",
            "|   NULL|Kolkata|           89|         73|\n",
            "+-------+-------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "branch1_new = branch1.withColumn(\"Science_marks\",lit(None))"
      ],
      "metadata": {
        "id": "b67CoOWnOXvF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = branch2.unionByName(branch1_new)"
      ],
      "metadata": {
        "id": "TvLxIBvzOoM3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0QImqZxQd5r",
        "outputId": "ae879bea-d51c-48dc-9820-5c752ce34d26"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Student', 'string'),\n",
              " ('Branch', 'string'),\n",
              " ('Science_marks', 'int'),\n",
              " ('Maths_marks', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name, col_types in merged.dtypes:\n",
        "  if col_types == 'int':\n",
        "    merged = merged.fillna({col_name:-9999})\n",
        "  elif col_types == 'string':\n",
        "    merged = merged.fillna({col_name:'unknown'})\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aw0RuSFIPJR7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS5Ew_YZRCG_",
        "outputId": "a6a79d4d-76ee-4bb1-dfa1-2494da09bfb0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------------+-----------+\n",
            "|Student| Branch|Science_marks|Maths_marks|\n",
            "+-------+-------+-------------+-----------+\n",
            "|   Arav|Kolkata|           79|         83|\n",
            "|unknown|Kolkata|           89|         73|\n",
            "|   Neha|  Delhi|        -9999|         90|\n",
            "+-------+-------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "data = [\n",
        " (101, [\"dosa\", \"biriyani\", \"idli\"]),\n",
        " (102, [\"biriyani\", \"mineral water\"]),\n",
        " (103, [\"rice\", \"mineral water\", \"poha\"]),\n",
        " (109, [\"idli\", \"biriyani\", \"poha\"]),\n",
        "]\n",
        "\n",
        "Schema = [\"bill_id\",\"food_item\"]"
      ],
      "metadata": {
        "id": "21gokEMoSKL6"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data= data, schema = Schema)"
      ],
      "metadata": {
        "id": "UZAhKTyqV5N2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate =False)"
      ],
      "metadata": {
        "id": "-9INhpaDWSfx",
        "outputId": "b4f7a4ab-a599-4ead-9a1c-928f4383a1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------+\n",
            "|bill_id|food_item                  |\n",
            "+-------+---------------------------+\n",
            "|101    |[dosa, biriyani, idli]     |\n",
            "|102    |[biriyani, mineral water]  |\n",
            "|103    |[rice, mineral water, poha]|\n",
            "|109    |[idli, biriyani, poha]     |\n",
            "+-------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.select(\"bill_id\",explode(col(\"food_item\")).alias(\"food_items\"))"
      ],
      "metadata": {
        "id": "saL_xEJEWhK3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.show()"
      ],
      "metadata": {
        "id": "PBfmUjVxWwFX",
        "outputId": "f5fe028e-86fb-4ccc-dad4-df149c725e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|bill_id|   food_items|\n",
            "+-------+-------------+\n",
            "|    101|         dosa|\n",
            "|    101|     biriyani|\n",
            "|    101|         idli|\n",
            "|    102|     biriyani|\n",
            "|    102|mineral water|\n",
            "|    103|         rice|\n",
            "|    103|mineral water|\n",
            "|    103|         poha|\n",
            "|    109|         idli|\n",
            "|    109|     biriyani|\n",
            "|    109|         poha|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (101, [\"dosa\", \"biriyani\", \"idli\"]),\n",
        " (102, [\"biriyani\", \"mineral water\"]),\n",
        " (103, [\"rice\", \"mineral water\", \"poha\"]),\n",
        " (109, [\"idli\", \"biriyani\", \"poha\"]),\n",
        "]\n",
        "\n",
        "Schema = [\"bill_id\",\"food_item\"]\n",
        "df = spark.createDataFrame(data= data, schema = Schema)\n",
        "df1 = df.select(\"bill_id\",explode(col(\"food_item\")).alias(\"food_items\"))\n",
        "df1.groupBy(\"food_items\").agg(count(\"food_items\").alias(\"count\")).orderBy(col(\"count\").desc()).show()"
      ],
      "metadata": {
        "id": "c2uIPBn8XKVv",
        "outputId": "b02dd19d-dad4-42c2-fd5a-a68c714d6bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|   food_items|count|\n",
            "+-------------+-----+\n",
            "|     biriyani|    3|\n",
            "|         idli|    2|\n",
            "|mineral water|    2|\n",
            "|         poha|    2|\n",
            "|         dosa|    1|\n",
            "|         rice|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\n",
        "('Rudra','math',79),\n",
        "('Rudra','eng',60),\n",
        "('Shivu','math', 68),\n",
        "('Shivu','eng', 59),\n",
        "('Anu','math', 65),\n",
        "('Anu','eng',80)\n",
        "]\n",
        "schema=\"Name string,Sub string,Marks int\"\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JxgKAZLWXUuy",
        "outputId": "ff9f2d7c-d4ea-45f7-bb9d-f6da40cbfd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-----+\n",
            "| Name| Sub|Marks|\n",
            "+-----+----+-----+\n",
            "|Rudra|math|   79|\n",
            "|Rudra| eng|   60|\n",
            "|Shivu|math|   68|\n",
            "|Shivu| eng|   59|\n",
            "|  Anu|math|   65|\n",
            "|  Anu| eng|   80|\n",
            "+-----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.groupBy(\"Name\").pivot(\"Sub\").agg(first(df.Marks))"
      ],
      "metadata": {
        "id": "Bg5Ocj-LYxxW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "HC8beh6EZUsa",
        "outputId": "e4bdf64e-4af8-4f06-fb54-a05c91569c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----+\n",
            "| Name|eng|math|\n",
            "+-----+---+----+\n",
            "|Shivu| 59|  68|\n",
            "|Rudra| 60|  79|\n",
            "|  Anu| 80|  65|\n",
            "+-----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [\n",
        " (1,'Neha' , 30 , None, 'IT'),\n",
        " (2,'Mark' , None , None, 'HR'),\n",
        " (3,'David' , 25 , None, 'HR'),\n",
        " (4,'Carol' , 30 , None, None)\n",
        "]"
      ],
      "metadata": {
        "id": "wuJNAj9SZWiN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp_schema = StructType([\n",
        " StructField(\"Id\" , IntegerType()) ,\n",
        " StructField(\"Name\" , StringType()) ,\n",
        " StructField(\"Age\" , IntegerType()) ,\n",
        " StructField(\"Salary\" , IntegerType()) ,\n",
        " StructField(\"Department\" , StringType()) ]\n",
        ")\n",
        "\n",
        "df= spark.createDataFrame(emp_data,emp_schema)"
      ],
      "metadata": {
        "id": "ETOrKs7pZ2nT"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "x_1-7WkwaGqF",
        "outputId": "f58fa356-2ffe-4e88-e81d-62fb9f1d68cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+------+----------+\n",
            "| Id| Name| Age|Salary|Department|\n",
            "+---+-----+----+------+----------+\n",
            "|  1| Neha|  30|  NULL|        IT|\n",
            "|  2| Mark|NULL|  NULL|        HR|\n",
            "|  3|David|  25|  NULL|        HR|\n",
            "|  4|Carol|  30|  NULL|      NULL|\n",
            "+---+-----+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "iTX_Es-oatMc",
        "outputId": "de89cae1-452e-4088-d3c7-caa5f392423e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Id', 'Name', 'Age', 'Salary', 'Department']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "# Provided data\n",
        "data = [\n",
        " (\"Alice\", 28),\n",
        " (\"Bob\", 35),\n",
        " (\"Charlie\", 42),\n",
        " (\"David\", 25),\n",
        " (\"Eva\", 31),\n",
        " (\"Frank\", 38),\n",
        " (\"Grace\", 45),\n",
        " (\"Henry\", 29)\n",
        "]\n",
        "schema = StructType([\n",
        " StructField(\"Name\", StringType(), True),\n",
        " StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "num_tiles=4"
      ],
      "metadata": {
        "id": "cg54NgxGav25"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wind_spec = Window.orderBy(lit(1))"
      ],
      "metadata": {
        "id": "h0_Z8RVBbLbh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"row_num\",row_number().over(wind_spec))"
      ],
      "metadata": {
        "id": "ocsRBJLgbeTr"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "0PwpemWubqki",
        "outputId": "ee13d8bb-174d-4eeb-9575-9d4c04fcf955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------+\n",
            "|   Name|Age|row_num|\n",
            "+-------+---+-------+\n",
            "|  Alice| 28|      1|\n",
            "|    Bob| 35|      2|\n",
            "|Charlie| 42|      3|\n",
            "|  David| 25|      4|\n",
            "|    Eva| 31|      5|\n",
            "|  Frank| 38|      6|\n",
            "|  Grace| 45|      7|\n",
            "|  Henry| 29|      8|\n",
            "+-------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.orderBy(col(\"row_num\"))"
      ],
      "metadata": {
        "id": "8euXuW3HbrzQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.withColumn(\"bucket\",ntile(num_tiles).over(window_spec))"
      ],
      "metadata": {
        "id": "VLdFBSMOb241"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "pTijSOyecGy0",
        "outputId": "080c3e64-3259-4691-c32b-7e238b3a6a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------+------+\n",
            "|   Name|Age|row_num|bucket|\n",
            "+-------+---+-------+------+\n",
            "|  Alice| 28|      1|     1|\n",
            "|    Bob| 35|      2|     1|\n",
            "|Charlie| 42|      3|     2|\n",
            "|  David| 25|      4|     2|\n",
            "|    Eva| 31|      5|     3|\n",
            "|  Frank| 38|      6|     3|\n",
            "|  Grace| 45|      7|     4|\n",
            "|  Henry| 29|      8|     4|\n",
            "+-------+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_df = df.filter(col(\"bucket\") == 3).drop(col(\"bucket\"),col(\"row_num\"))"
      ],
      "metadata": {
        "id": "36qiStAJcH4t"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_df.show()"
      ],
      "metadata": {
        "id": "gTJL5E10cPTc",
        "outputId": "daec5dc6-3a5d-41ae-b424-97ed86f29b70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|  Eva| 31|\n",
            "|Frank| 38|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seat_data = [\n",
        " (1 , 'Abbot'),\n",
        " (2, 'Doris'),\n",
        " (3, 'Emerson' ),\n",
        " (4, 'Green'),\n",
        " (5,'Jeames' )\n",
        "]\n",
        "\n",
        "seat_schema = ['id','Name']\n",
        "\n",
        "df = spark.createDataFrame(seat_data,seat_schema)"
      ],
      "metadata": {
        "id": "4YePvYOQcmNq"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "cjki47MPd07y",
        "outputId": "80775070-f5f2-44bb-ac9b-c217b677bb96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   Name|\n",
            "+---+-------+\n",
            "|  1|  Abbot|\n",
            "|  2|  Doris|\n",
            "|  3|Emerson|\n",
            "|  4|  Green|\n",
            "|  5| Jeames|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"previous\",lag(\"Name\").over(Window.orderBy(\"id\"))) \\\n",
        ".withColumn(\"next\",lead(\"Name\").over(Window.orderBy(\"id\"))).show()"
      ],
      "metadata": {
        "id": "4V6ABQhWd5_1",
        "outputId": "ccec36ee-aec8-417f-da14-610a186bdda4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+-------+\n",
            "| id|   Name|previous|   next|\n",
            "+---+-------+--------+-------+\n",
            "|  1|  Abbot|    NULL|  Doris|\n",
            "|  2|  Doris|   Abbot|Emerson|\n",
            "|  3|Emerson|   Doris|  Green|\n",
            "|  4|  Green| Emerson| Jeames|\n",
            "|  5| Jeames|   Green|   NULL|\n",
            "+---+-------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.withColumn(\"exchange\",coalesce(\n",
        "    when(col(\"id\")%2==0,df.previous)\\\n",
        "    .when(col(\"id\")%2==1,df.next),df.name\n",
        "))"
      ],
      "metadata": {
        "id": "3gdY394MelvT",
        "outputId": "366655be-de30-4da7-f647-c1c0a8c3d961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'previous'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-4efcdc5a801f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df1 = df.withColumn(\"exchange\",coalesce(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3121\u001b[0m         \"\"\"\n\u001b[1;32m   3122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3123\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3124\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3125\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'previous'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYBwPjgPfXPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}